# train_model.py
# File n√†y k·∫øt h·ª£p ƒë·ªãnh nghƒ©a m√¥ h√¨nh v√† hu·∫•n luy·ªán

import os
import argparse
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import json
import datetime
from tensorflow.keras.applications import DenseNet121, ResNet50V2
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from sklearn.metrics import confusion_matrix, roc_curve, auc

# Ki·ªÉm tra phi√™n b·∫£n
print(f"TensorFlow version: {tf.__version__}")

# Khai b√°o c√°c ƒë∆∞·ªùng d·∫´n
os.makedirs('models', exist_ok=True)
os.makedirs('results', exist_ok=True)

# T·∫°o c√°c m√¥ h√¨nh
def create_densenet121_model(input_shape=(224, 224, 3), weights='imagenet'):
    """T·∫°o m√¥ h√¨nh DenseNet121 v·ªõi input RGB"""
    # Base model
    base_model = DenseNet121(
        weights=weights,
        include_top=False,
        input_shape=input_shape
    )
    
    # ƒê√≥ng bƒÉng c√°c l·ªõp
    for layer in base_model.layers:
        layer.trainable = False
    
    # T·∫°o m√¥ h√¨nh tu·∫ßn t·ª±
    model = tf.keras.Sequential([
        base_model,
        GlobalAveragePooling2D(),
        Dense(512, activation='relu'),
        Dropout(0.3),
        Dense(128, activation='relu'),
        Dropout(0.2),
        Dense(1, activation='sigmoid')
    ])
    
    return model, base_model

def create_resnet50v2_model(input_shape=(224, 224, 3), weights='imagenet'):
    """T·∫°o m√¥ h√¨nh ResNet50V2 v·ªõi input RGB"""
    # Base model
    base_model = ResNet50V2(
        weights=weights,
        include_top=False,
        input_shape=input_shape
    )
    
    # ƒê√≥ng bƒÉng c√°c l·ªõp
    for layer in base_model.layers:
        layer.trainable = False
    
    # T·∫°o m√¥ h√¨nh tu·∫ßn t·ª±
    model = tf.keras.Sequential([
        base_model,
        GlobalAveragePooling2D(),
        Dense(512, activation='relu'),
        Dropout(0.3),
        Dense(128, activation='relu'),
        Dropout(0.2),
        Dense(1, activation='sigmoid')
    ])
    
    return model, base_model

def create_confusion_matrix(y_true, y_pred, class_names, title, filename):
    """T·∫°o v√† l∆∞u confusion matrix"""
    cm = confusion_matrix(y_true, y_pred)
    
    plt.figure(figsize=(8, 6))
    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    plt.title(title)
    plt.colorbar()
    
    tick_marks = np.arange(len(class_names))
    plt.xticks(tick_marks, class_names)
    plt.yticks(tick_marks, class_names)
    
    # Th√™m text v√†o c√°c √¥
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            plt.text(j, i, format(cm[i, j], 'd'),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")
    
    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.savefig(filename)
    plt.close()

def create_roc_curve(y_true, y_pred_prob, title, filename):
    """T·∫°o v√† l∆∞u ƒë∆∞·ªùng cong ROC"""
    fpr, tpr, _ = roc_curve(y_true, y_pred_prob)
    roc_auc = auc(fpr, tpr)
    
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(title)
    plt.legend(loc="lower right")
    plt.savefig(filename)
    plt.close()

# Callback t√πy ch·ªânh ƒë·ªÉ l∆∞u th√¥ng tin epoch
class SaveEpochInfo(tf.keras.callbacks.Callback):
    def __init__(self, filepath):
        super(SaveEpochInfo, self).__init__()
        self.filepath = filepath
        
    def on_epoch_end(self, epoch, logs=None):
        with open(self.filepath, 'w') as f:
            json.dump({
                'epoch': epoch + 1,
                'date': datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'val_loss': float(logs.get('val_loss', 0)),
                'val_accuracy': float(logs.get('val_accuracy', 0)),
                'val_auc': float(logs.get('val_auc', 0))
            }, f)

def train_model(base_dir, target_region, model_name, img_size=(224, 224), batch_size=32, epochs=50, initial_epoch=0, continue_model=None):
    """Hu·∫•n luy·ªán m√¥ h√¨nh"""
    print("\n" + "="*50)
    print(f"{'TI·∫æP T·ª§C' if initial_epoch > 0 else 'B·∫ÆT ƒê·∫¶U'} HU·∫§N LUY·ªÜN M√î H√åNH {model_name.upper()} CHO V√ôNG {target_region}")
    if initial_epoch > 0:
        print(f"Ti·∫øp t·ª•c t·ª´ epoch {initial_epoch}/{epochs}")
    print("="*50)
    
    # Ki·ªÉm tra n·∫øu base_dir kh√¥ng ph·∫£i l√† ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi
    if not os.path.isabs(base_dir):
        print(f"ƒê∆∞·ªùng d·∫´n ƒë·∫ßu v√†o: {base_dir}")
        # Gi·∫£ s·ª≠ mura_final n·∫±m trong th∆∞ m·ª•c MURA
        base_dir = os.path.join("C:\\Users\\Admin\\Downloads\\fonai\\MURA", base_dir)
        print(f"ƒê√£ ƒëi·ªÅu ch·ªânh ƒë∆∞·ªùng d·∫´n th√†nh: {base_dir}")
    
    # ƒê∆∞·ªùng d·∫´n d·ªØ li·ªáu
    train_dir = os.path.join(base_dir, target_region, 'train')
    val_dir = os.path.join(base_dir, target_region, 'val')
    test_dir = os.path.join(base_dir, target_region, 'test')
    
    # In ƒë∆∞·ªùng d·∫´n ƒë·∫ßy ƒë·ªß ƒë·ªÉ debug
    print(f"\nƒê∆Ø·ªúNG D·∫™N ƒê·∫¶Y ƒê·ª¶:")
    print(f"- Train: {train_dir}")
    print(f"- Validation: {val_dir}")   
    print(f"- Test: {test_dir}")
    
    # Ki·ªÉm tra th∆∞ m·ª•c v√† hi·ªÉn th·ªã th√¥ng tin chi ti·∫øt
    missing_dirs = []
    for directory_type, directory_path in [("Train", train_dir), ("Val", val_dir), ("Test", test_dir)]:
        if os.path.exists(directory_path):
            print(f"Th∆∞ m·ª•c {directory_type} t·ªìn t·∫°i: ‚úì")
            # Ki·ªÉm tra c√°c th∆∞ m·ª•c con
            abnormal_dir = os.path.join(directory_path, 'abnormal')
            normal_dir = os.path.join(directory_path, 'normal')
            
            if os.path.exists(abnormal_dir) and os.path.exists(normal_dir):
                print(f"  - Th∆∞ m·ª•c 'abnormal' v√† 'normal' t·ªìn t·∫°i: ‚úì")
                # ƒê·∫øm s·ªë l∆∞·ª£ng file trong m·ªói th∆∞ m·ª•c
                abnormal_files = len([f for f in os.listdir(abnormal_dir) if os.path.isfile(os.path.join(abnormal_dir, f))])
                normal_files = len([f for f in os.listdir(normal_dir) if os.path.isfile(os.path.join(normal_dir, f))])
                print(f"  - S·ªë l∆∞·ª£ng ·∫£nh: abnormal={abnormal_files}, normal={normal_files}")
            else:
                if not os.path.exists(abnormal_dir):
                    print(f"  - Th∆∞ m·ª•c 'abnormal' KH√îNG t·ªìn t·∫°i trong {directory_type}: ‚úó")
                    missing_dirs.append(abnormal_dir)
                if not os.path.exists(normal_dir):
                    print(f"  - Th∆∞ m·ª•c 'normal' KH√îNG t·ªìn t·∫°i trong {directory_type}: ‚úó")
                    missing_dirs.append(normal_dir)
        else:
            print(f"Th∆∞ m·ª•c {directory_type} KH√îNG t·ªìn t·∫°i: ‚úó")
            missing_dirs.append(directory_path)
            # Ki·ªÉm tra xem th∆∞ m·ª•c cha c√≥ t·ªìn t·∫°i kh√¥ng
            parent_dir = os.path.dirname(directory_path)
            if os.path.exists(parent_dir):
                print(f"  - Th∆∞ m·ª•c cha '{os.path.basename(parent_dir)}' t·ªìn t·∫°i: ‚úì")
                # Li·ªát k√™ c√°c th∆∞ m·ª•c con trong th∆∞ m·ª•c cha
                sub_dirs = [d for d in os.listdir(parent_dir) if os.path.isdir(os.path.join(parent_dir, d))]
                print(f"  - C√°c th∆∞ m·ª•c con c√≥ s·∫µn: {', '.join(sub_dirs) if sub_dirs else 'Kh√¥ng c√≥'}")
            else:
                print(f"  - Th∆∞ m·ª•c cha '{os.path.basename(parent_dir)}' KH√îNG t·ªìn t·∫°i: ‚úó")
    
    if missing_dirs:
        print("\nTH∆Ø M·ª§C KH√îNG T·ªíN T·∫†I:")
        for dir_path in missing_dirs:
            print(f" - {dir_path}")
        
        print("\nƒê∆Ø·ªúNG D·∫™N HI·ªÜN T·∫†I:", os.getcwd())
        print(f"\nC·∫§U TR√öC CH√çNH X√ÅC C·∫¶N C√ì:")
        print(f"{base_dir}\\{target_region}\\train\\abnormal")
        print(f"{base_dir}\\{target_region}\\train\\normal")
        print(f"{base_dir}\\{target_region}\\val\\abnormal")
        print(f"{base_dir}\\{target_region}\\val\\normal")
        print(f"{base_dir}\\{target_region}\\test\\abnormal")
        print(f"{base_dir}\\{target_region}\\test\\normal")
        
        print("\nVui l√≤ng ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n ho·∫∑c t·∫°o c√°c th∆∞ m·ª•c tr√™n.")
        raise ValueError(f"Kh√¥ng th·ªÉ ti·∫øp t·ª•c v√¨ thi·∫øu {len(missing_dirs)} th∆∞ m·ª•c.")
    
    # Ti·∫øp t·ª•c n·∫øu c√°c th∆∞ m·ª•c ƒë·ªÅu t·ªìn t·∫°i
    print("\nT·∫•t c·∫£ c√°c th∆∞ m·ª•c ƒë·ªÅu t·ªìn t·∫°i, ti·∫øp t·ª•c hu·∫•n luy·ªán...\n")
    
    # Data augmentation cho t·∫≠p train
    train_datagen = ImageDataGenerator(
        rescale=1./255,
        rotation_range=15,
        width_shift_range=0.1,
        height_shift_range=0.1,
        shear_range=0.1,
        zoom_range=0.1,
        horizontal_flip=True,
        fill_mode='nearest'
    )
    
    # Ch·ªâ rescale cho validation v√† test
    val_datagen = ImageDataGenerator(rescale=1./255)
    test_datagen = ImageDataGenerator(rescale=1./255)
    
    try:
        # T·∫°o generators v·ªõi color_mode='rgb'
        train_generator = train_datagen.flow_from_directory(
            train_dir,
            target_size=img_size,
            batch_size=batch_size,
            class_mode='binary',
            shuffle=True,
            color_mode='rgb'  # Thay ƒë·ªïi t·ª´ grayscale sang rgb
        )
        
        validation_generator = val_datagen.flow_from_directory(
            val_dir,
            target_size=img_size,
            batch_size=batch_size,
            class_mode='binary',
            shuffle=False,
            color_mode='rgb'  # Thay ƒë·ªïi t·ª´ grayscale sang rgb
        )
        
        test_generator = test_datagen.flow_from_directory(
            test_dir,
            target_size=img_size,
            batch_size=batch_size,
            class_mode='binary',
            shuffle=False,
            color_mode='rgb'  # Thay ƒë·ªïi t·ª´ grayscale sang rgb
        )
    except Exception as e:
        print(f"\nL·ªñI KHI T·∫†O DATA GENERATORS: {str(e)}")
        print("Vui l√≤ng ki·ªÉm tra l·∫°i c·∫•u tr√∫c th∆∞ m·ª•c v√† ƒë·ªãnh d·∫°ng ·∫£nh.")
        return None, None
    
    # Th√¥ng tin v·ªÅ d·ªØ li·ªáu
    print(f"T·ªïng s·ªë m·∫´u train: {train_generator.samples}")
    print(f"T·ªïng s·ªë m·∫´u validation: {validation_generator.samples}")
    print(f"T·ªïng s·ªë m·∫´u test: {test_generator.samples}")
    print(f"Mapping c√°c l·ªõp: {train_generator.class_indices}")
    
    # T√≠nh class weights
    total_samples = train_generator.samples
    n_positive = np.sum(train_generator.classes)
    n_negative = total_samples - n_positive
    
    weight_for_0 = (1 / n_negative) * (total_samples / 2.0)
    weight_for_1 = (1 / n_positive) * (total_samples / 2.0)
    class_weight = {0: weight_for_0, 1: weight_for_1}
    print(f"Class weights: {class_weight}")
    
    # T·∫°o ho·∫∑c n·∫°p l·∫°i m√¥ h√¨nh
    if continue_model is not None:
        print(f"Ti·∫øp t·ª•c hu·∫•n luy·ªán m√¥ h√¨nh ƒë√£ c√≥ t·ª´ epoch {initial_epoch}...")
        model = continue_model
        base_model = None  # Kh√¥ng c·∫ßn base_model khi ti·∫øp t·ª•c hu·∫•n luy·ªán
    else:
        # T·∫°o m√¥ h√¨nh v·ªõi input RGB
        print(f"ƒêang t·∫°o m√¥ h√¨nh {model_name} m·ªõi...")
        try:
            if model_name.lower() == 'densenet121':
                model, base_model = create_densenet121_model(input_shape=(*img_size, 3))
            elif model_name.lower() == 'resnet50v2':
                model, base_model = create_resnet50v2_model(input_shape=(*img_size, 3))
            else:
                raise ValueError(f"Kh√¥ng h·ªó tr·ª£ m√¥ h√¨nh: {model_name}")
        except Exception as e:
            print(f"\nL·ªñI KHI T·∫†O M√î H√åNH: {str(e)}")
            return None, None
        
        # Bi√™n d·ªãch m√¥ h√¨nh (ch·ªâ khi t·∫°o m√¥ h√¨nh m·ªõi)
        model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
            loss='binary_crossentropy',
            metrics=[
                'accuracy',
                tf.keras.metrics.AUC(name='auc'),
                tf.keras.metrics.Precision(name='precision'),
                tf.keras.metrics.Recall(name='recall')
            ]
        )
    
    # Hi·ªÉn th·ªã t√≥m t·∫Øt m√¥ h√¨nh
    model.summary()
    
    # Callbacks
    early_stopping = EarlyStopping(
        monitor='val_loss',
        patience=10,
        restore_best_weights=True
    )
    
    model_path = f"models/{model_name}_{target_region}_best.h5"
    checkpoint = ModelCheckpoint(
        model_path,
        monitor='val_auc',
        save_best_only=True,
        mode='max'
    )
    
    reduce_lr = ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.2,
        patience=5,
        min_lr=1e-6
    )
    
    # Callback l∆∞u th√¥ng tin ti·∫øn tr√¨nh
    checkpoint_info_path = f"models/{model_name}_{target_region}_checkpoint_info.json"
    save_epoch_info = SaveEpochInfo(checkpoint_info_path)
    
    # Danh s√°ch callbacks
    callbacks = [early_stopping, checkpoint, reduce_lr, save_epoch_info]
    
    # Hu·∫•n luy·ªán m√¥ h√¨nh (phase 1)
    print(f"\nB·∫Øt ƒë·∫ßu hu·∫•n luy·ªán (phase 1) t·ª´ epoch {initial_epoch}...")
    try:
        history = model.fit(
            train_generator,
            steps_per_epoch=train_generator.samples // batch_size,
            validation_data=validation_generator,
            validation_steps=validation_generator.samples // batch_size,
            epochs=epochs,
            initial_epoch=initial_epoch,  # B·∫Øt ƒë·∫ßu t·ª´ epoch ƒë√£ c√≥
            callbacks=callbacks,
            class_weight=class_weight
        )
    except Exception as e:
        print(f"\nL·ªñI TRONG QU√Å TR√åNH HU·∫§N LUY·ªÜN PHASE 1: {str(e)}")
        return model, None
    
    # B·ªè qua phase 2 (fine-tuning) t·∫°m th·ªùi ƒë·ªÉ ki·ªÉm tra xem phase 1 c√≥ ch·∫°y ƒë∆∞·ª£c kh√¥ng
    
    # L∆∞u m√¥ h√¨nh cu·ªëi c√πng
    final_model_path = f"models/{model_name}_{target_region}_final.h5"
    try:
        model.save(final_model_path)
        print(f"ƒê√£ l∆∞u m√¥ h√¨nh t·∫°i {final_model_path}")
    except Exception as e:
        print(f"L·ªói khi l∆∞u m√¥ h√¨nh: {str(e)}")
    
    # ƒê√°nh gi√° tr√™n t·∫≠p test
    print("\nƒê√°nh gi√° tr√™n t·∫≠p test:")
    try:
        test_results = model.evaluate(test_generator)
        metric_names = ['loss', 'accuracy', 'auc', 'precision', 'recall']
        print("Test Results:")
        for name, value in zip(metric_names, test_results):
            print(f"{name}: {value:.4f}")
        
        # V·∫Ω bi·ªÉu ƒë·ªì accuracy v√† loss
        plt.figure(figsize=(12, 5))
        
        # Accuracy
        plt.subplot(1, 2, 1)
        plt.plot(history.history['accuracy'])
        plt.plot(history.history['val_accuracy'])
        plt.title(f'Model Accuracy - {model_name} - {target_region}')
        plt.ylabel('Accuracy')
        plt.xlabel('Epoch')
        plt.legend(['Train', 'Validation'], loc='lower right')
        
        # Loss
        plt.subplot(1, 2, 2)
        plt.plot(history.history['loss'])
        plt.plot(history.history['val_loss'])
        plt.title(f'Model Loss - {model_name} - {target_region}')
        plt.ylabel('Loss')
        plt.xlabel('Epoch')
        plt.legend(['Train', 'Validation'], loc='upper right')
        
        plt.tight_layout()
        plt.savefig(f'results/training_history_{model_name}_{target_region}.png')
        plt.close()
        
        # D·ª± ƒëo√°n v√† t·∫°o confusion matrix
        y_pred = model.predict(test_generator)
        y_pred_classes = (y_pred > 0.5).astype(int).flatten()
        y_true = test_generator.classes
        
        # T·∫°o confusion matrix
        create_confusion_matrix(
            y_true, 
            y_pred_classes, 
            ['Normal', 'Abnormal'],
            f'Confusion Matrix - {model_name} - {target_region}',
            f'results/confusion_matrix_{model_name}_{target_region}.png'
        )
        
        # T·∫°o ROC curve
        create_roc_curve(
            y_true, 
            y_pred,
            f'ROC Curve - {model_name} - {target_region}',
            f'results/roc_curve_{model_name}_{target_region}.png'
        )
    except Exception as e:
        print(f"\nL·ªñI TRONG QU√Å TR√åNH ƒê√ÅNH GI√Å M√î H√åNH: {str(e)}")
    
    print(f"\nƒê√£ ho√†n th√†nh hu·∫•n luy·ªán v√† ƒë√°nh gi√° m√¥ h√¨nh {model_name} cho v√πng {target_region}")
    return model, history

def train_all_regions(model_name, batch_size, epochs, base_dir):
    """Hu·∫•n luy·ªán m√¥ h√¨nh cho c√°c v√πng ch∆∞a ho√†n th√†nh ho·∫∑c b·ªã gi√°n ƒëo·∫°n"""

    regions = [
        "XR_HAND", "XR_WRIST", "XR_ELBOW", "XR_FINGER", 
        "XR_FOREARM", "XR_HUMERUS", "XR_SHOULDER"
    ]
    
    results = {}
    for region in regions:
        model_path = f"models/{model_name}_{region}_best.h5"
        results_path = f"results/roc_curve_{model_name}_{region}.png"
        final_model_path = f"models/{model_name}_{region}_final.h5"
        checkpoint_info_path = f"models/{model_name}_{region}_checkpoint_info.json"

        # Ki·ªÉm tra n·∫øu ƒë√£ ho√†n th√†nh ƒë·∫ßy ƒë·ªß
        if os.path.exists(model_path) and os.path.exists(results_path) and os.path.exists(final_model_path):
            print(f"‚úÖ B·ªé QUA: {region} (ƒë√£ ho√†n th√†nh ƒë·∫ßy ƒë·ªß)")
            continue  # Skip v√πng ƒë√£ hu·∫•n luy·ªán v√† c√≥ k·∫øt qu·∫£ ƒë·∫ßy ƒë·ªß
        
        # Ki·ªÉm tra n·∫øu c√≥ checkpoint ƒë·ªÉ ti·∫øp t·ª•c
        initial_epoch = 0
        continue_model = None
        
        if os.path.exists(model_path) and os.path.exists(checkpoint_info_path):
            try:
                with open(checkpoint_info_path, 'r') as f:
                    info = json.load(f)
                    initial_epoch = info.get('epoch', 0)
                
                if initial_epoch > 0 and initial_epoch < epochs:
                    print(f"‚ö†Ô∏è TI·∫æP T·ª§C: {region} t·ª´ epoch {initial_epoch}/{epochs}")
                    # N·∫°p l·∫°i m√¥ h√¨nh t·ª´ checkpoint
                    continue_model = tf.keras.models.load_model(model_path)
                else:
                    # N·∫øu ƒë√£ ƒë·ªß epochs nh∆∞ng kh√¥ng c√≥ k·∫øt qu·∫£, hu·∫•n luy·ªán l·∫°i
                    print(f"‚ö†Ô∏è PH√ÅT HI·ªÜN: {region} ƒë√£ ho√†n th√†nh {initial_epoch} epochs nh∆∞ng kh√¥ng c√≥ ƒë·ªß k·∫øt qu·∫£. Hu·∫•n luy·ªán l·∫°i...")
                    initial_epoch = 0
            except Exception as e:
                print(f"L·ªói khi n·∫°p checkpoint: {str(e)}")
                initial_epoch = 0
        elif os.path.exists(model_path):
            print(f"‚ö†Ô∏è PH√ÅT HI·ªÜN: {region} c√≥ file model nh∆∞ng kh√¥ng c√≥ th√¥ng tin checkpoint. Hu·∫•n luy·ªán l·∫°i...")
        
        print(f"\nüîÅ {'TI·∫æP T·ª§C' if initial_epoch > 0 else 'B·∫ÆT ƒê·∫¶U'} HU·∫§N LUY·ªÜN CHO V√ôNG: {region}")
        
        # Truy·ªÅn initial_epoch v√† model (n·∫øu c√≥) v√†o h√†m train_model
        model, history = train_model(
            base_dir, region, model_name,
            img_size=(224, 224),
            batch_size=batch_size,
            epochs=epochs,
            initial_epoch=initial_epoch,
            continue_model=continue_model
        )
        
        if history:
            results[region] = {
                'accuracy': history.history.get('val_accuracy', [-1])[-1],
                'auc': history.history.get('val_auc', [-1])[-1]
            }

    # T·ªïng k·∫øt k·∫øt qu·∫£
    print("\n" + "="*60)
    print(f"T√ìM T·∫ÆT K·∫æT QU·∫¢ HU·∫§N LUY·ªÜN M√î H√åNH {model_name.upper()}")
    print("="*60)
    for region, metrics in results.items():
        acc = metrics['accuracy']
        auc_score = metrics['auc']
        print(f"{region}: Accuracy = {acc:.4f}, AUC = {auc_score:.4f}")
    print("="*60)

    
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Hu·∫•n luy·ªán m√¥ h√¨nh ph√°t hi·ªán g√£y x∆∞∆°ng')
    parser.add_argument('--base_dir', type=str, default='C:\\Users\\Admin\\Downloads\\fonai\\MURA\\mura_final',
                      help='ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c g·ªëc ch·ª©a d·ªØ li·ªáu')
    parser.add_argument('--model', type=str, choices=['densenet121', 'resnet50v2', 'both'],
                      default='densenet121', help='Ki·∫øn tr√∫c m√¥ h√¨nh')
    parser.add_argument('--batch_size', type=int, default=8, help='K√≠ch th∆∞·ªõc batch')
    parser.add_argument('--epochs', type=int, default=10, help='S·ªë epoch')
    parser.add_argument('--img_size', type=int, default=224, help='K√≠ch th∆∞·ªõc ·∫£nh')
    
    args = parser.parse_args()
    
    print("\n" + "="*60)
    print("TH√îNG TIN HU·∫§N LUY·ªÜN:")
    print(f"- Th∆∞ m·ª•c d·ªØ li·ªáu: {args.base_dir}")
    print(f"- M√¥ h√¨nh: {args.model}")
    print(f"- Batch size: {args.batch_size}")
    print(f"- S·ªë epoch: {args.epochs}")
    print(f"- K√≠ch th∆∞·ªõc ·∫£nh: {args.img_size}x{args.img_size}")
    print("="*60 + "\n")
    
    # Ki·ªÉm tra xem c√°c th∆∞ m·ª•c output c√≥ t·ªìn t·∫°i kh√¥ng
    for output_dir in ['models', 'results']:
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
            print(f"ƒê√£ t·∫°o th∆∞ m·ª•c '{output_dir}'")
    
    # Hu·∫•n luy·ªán d·ª±a v√†o m√¥ h√¨nh ƒë∆∞·ª£c ch·ªçn
    if args.model == 'both':
        print("=== HU·∫§N LUY·ªÜN C·∫¢ HAI M√î H√åNH CHO T·∫§T C·∫¢ C√ÅC V√ôNG ===")
        
        print("\n=== HU·∫§N LUY·ªÜN DENSENET121 CHO T·∫§T C·∫¢ C√ÅC V√ôNG ===")
        train_all_regions('densenet121', args.batch_size, args.epochs, args.base_dir)
        
        print("\n=== HU·∫§N LUY·ªÜN RESNET50V2 CHO T·∫§T C·∫¢ C√ÅC V√ôNG ===")
        train_all_regions('resnet50v2', args.batch_size, args.epochs, args.base_dir)
    else:
        print(f"=== HU·∫§N LUY·ªÜN {args.model.upper()} CHO T·∫§T C·∫¢ C√ÅC V√ôNG ===")
        train_all_regions(args.model, args.batch_size, args.epochs, args.base_dir)